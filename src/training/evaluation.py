import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple, Union, Callable, Any
from sklearn.metrics import (
    roc_auc_score, 
    precision_recall_curve, 
    average_precision_score,
    auc,
    precision_score,
    recall_score,
    f1_score,
    roc_curve
)
import os
import logging
from tqdm import tqdm
import random

from src.data.datasets import GraphData
from src.models.predictors.base import BasePredictor


class EvaluationFramework:
    """
    A comprehensive framework for evaluating and comparing predictor models.
    
    This framework provides standardized metrics and evaluation methods for
    various prediction tasks in citation networks, including:
    - Link prediction
    - Temporal link prediction
    - Cross-validation
    - Ablation studies
    
    It also includes visualization tools for comparing predictor performance.
    """
    
    def __init__(self, 
                 metrics: Optional[List[str]] = None,
                 k_values: Optional[List[int]] = None,
                 output_dir: str = './evaluation_results'):
        """
        Initialize the evaluation framework.
        
        Args:
            metrics: List of metrics to compute. If None, uses all available metrics.
            k_values: List of k values for Precision@k, Recall@k metrics.
            output_dir: Directory to save evaluation results and plots.
        """
        self.metrics = metrics or ['auc', 'ap', 'precision', 'recall', 'f1', 'mrr']
        self.k_values = k_values or [1, 5, 10, 20, 50, 100]
        self.output_dir = output_dir
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Set up logger
        self.logger = logging.getLogger(__name__)
        
        # Predictor comparison results
        self.comparison_results = {}
        
        # Cache for metric computation
        self.metrics_cache = {}
        self.max_cache_size = 100  # Limit cache size to prevent memory leaks
        self.cache_keys = []  # To track cache entry order (for LRU eviction)
    
    def evaluate_predictor(self, 
                          predictor: BasePredictor,
                          graph_data: GraphData,
                          node_embeddings: torch.Tensor,
                          split_type: str = 'random',
                          test_ratio: float = 0.2,
                          time_threshold: Optional[float] = None,
                          future_window: Optional[float] = None,
                          k_fold: int = 5,
                          **kwargs) -> Dict[str, float]:
        """
        Evaluate a predictor using the specified data and split method.
        
        Args:
            predictor: The predictor model to evaluate
            graph_data: The citation network data
            node_embeddings: Node embeddings generated by an encoder
            split_type: Type of data split ('random', 'temporal', 'k-fold')
            test_ratio: Ratio of edges to use for testing (for 'random' split)
            time_threshold: Timestamp to use as cutoff (for 'temporal' split)
            future_window: Time window after threshold to predict (for 'temporal' split)
            k_fold: Number of folds (for 'k-fold' split)
            **kwargs: Additional arguments for specific evaluation methods
            
        Returns:
            Dictionary containing evaluation metrics
        """
        # Choose the appropriate evaluation method based on split type
        if split_type == 'random':
            return self._evaluate_random_split(
                predictor, graph_data, node_embeddings, test_ratio, **kwargs
            )
        elif split_type == 'temporal':
            if time_threshold is None:
                raise ValueError("time_threshold must be provided for temporal split")
            return self._evaluate_temporal_split(
                predictor, graph_data, node_embeddings, 
                time_threshold, future_window, **kwargs
            )
        elif split_type == 'k-fold':
            return self._evaluate_k_fold(
                predictor, graph_data, node_embeddings, k_fold, **kwargs
            )
        else:
            raise ValueError(f"Unknown split type: {split_type}")
    
    def _evaluate_random_split(self,
                              predictor: BasePredictor,
                              graph_data: GraphData,
                              node_embeddings: torch.Tensor,
                              test_ratio: float = 0.2,
                              **kwargs) -> Dict[str, float]:
        """
        Evaluate predictor using a random split of edges.
        
        Args:
            predictor: The predictor model to evaluate
            graph_data: The citation network data
            node_embeddings: Node embeddings generated by an encoder
            test_ratio: Ratio of edges to use for testing
            **kwargs: Additional arguments
            
        Returns:
            Dictionary containing evaluation metrics
        """
        # Get all edges from the graph
        all_edges = graph_data.edge_index
        num_edges = all_edges.shape[1]
        
        # Randomly split edges into train and test sets
        indices = torch.randperm(num_edges)
        test_size = int(num_edges * test_ratio)
        test_indices = indices[:test_size]
        train_indices = indices[test_size:]
        
        # Create train and test edge sets
        train_edges = all_edges[:, train_indices]
        test_edges = all_edges[:, test_indices]
        
        # Create a modified graph with only training edges
        train_graph = self._create_subgraph(graph_data, train_edges)
        
        # Generate negative samples for testing
        # (edges that don't exist in the full graph)
        negative_edges = self._generate_negative_edges(
            graph_data, 
            num_samples=test_edges.shape[1],
            exclude_edges=all_edges
        )
        
        # Prepare positive and negative examples for evaluation
        src_pos, dst_pos = test_edges[0], test_edges[1]
        src_neg, dst_neg = negative_edges[0], negative_edges[1]
        
        # Create edge attributes if available (e.g., time differences)
        pos_edge_attr = None
        neg_edge_attr = None
        
        if hasattr(graph_data, 'edge_attr') and graph_data.edge_attr is not None:
            pos_edge_attr = graph_data.edge_attr[test_indices]
            
            # For negative edges, we can create placeholder attributes
            # or compute them based on node information
            # This is a simplified approach; in practice, you might
            # want to generate appropriate attributes
            neg_edge_attr = torch.zeros(
                (negative_edges.shape[1], graph_data.edge_attr.shape[1]),
                device=graph_data.edge_attr.device
            )
        
        # Get embeddings for source and destination nodes
        src_pos_emb = node_embeddings[src_pos]
        dst_pos_emb = node_embeddings[dst_pos]
        src_neg_emb = node_embeddings[src_neg]
        dst_neg_emb = node_embeddings[dst_neg]
        
        # Predict scores for positive and negative edges
        pos_scores = predictor(src_pos_emb, dst_pos_emb, pos_edge_attr)
        neg_scores = predictor(src_neg_emb, dst_neg_emb, neg_edge_attr)
        
        # Prepare ground truth labels
        pos_labels = torch.ones_like(pos_scores)
        neg_labels = torch.zeros_like(neg_scores)
        
        # Combine predictions and labels
        scores = torch.cat([pos_scores, neg_scores], dim=0)
        labels = torch.cat([pos_labels, neg_labels], dim=0)
        
        # Compute evaluation metrics
        metrics = self._compute_metrics(scores, labels)
        
        return metrics
    
    def _evaluate_temporal_split(self,
                                predictor: BasePredictor,
                                graph_data: GraphData,
                                node_embeddings: torch.Tensor,
                                time_threshold: float,
                                future_window: Optional[float] = None,
                                **kwargs) -> Dict[str, float]:
        """
        Evaluate predictor using a temporal split of edges.
        
        Uses edges before time_threshold for training and edges after for testing.
        
        Args:
            predictor: The predictor model to evaluate
            graph_data: The citation network data
            node_embeddings: Node embeddings generated by an encoder
            time_threshold: Timestamp to use as cutoff
            future_window: Optional time window after threshold to predict
            **kwargs: Additional arguments
            
        Returns:
            Dictionary containing evaluation metrics
        """
        # Get edge timestamps
        if hasattr(graph_data, 'edge_timestamps') and graph_data.edge_timestamps is not None:
            edge_times = graph_data.edge_timestamps
        elif hasattr(graph_data, 'edge_time') and graph_data.edge_time is not None:
            edge_times = graph_data.edge_time
        else:
            raise ValueError("Graph data must contain edge timestamps for temporal evaluation")
        
        # Get all edges from the graph
        all_edges = graph_data.edge_index
        
        # Find edges before and after the time threshold
        before_mask = edge_times <= time_threshold
        
        if future_window is not None:
            # Only include edges within the future window
            after_mask = (edge_times > time_threshold) & (edge_times <= time_threshold + future_window)
        else:
            # Include all future edges
            after_mask = edge_times > time_threshold
        
        # Split edges into train (before) and test (after) sets
        train_edges = all_edges[:, before_mask]
        test_edges = all_edges[:, after_mask]
        
        # Check if we have enough test edges
        if test_edges.shape[1] == 0:
            self.logger.warning(f"No test edges found after time {time_threshold}. "
                               f"Consider using a lower threshold or removing the future window.")
            # Return empty metrics
            return {metric: float('nan') for metric in self.metrics}
        
        # Create a modified graph with only training edges
        train_graph = self._create_subgraph(graph_data, train_edges)
        
        # Copy the timestamps to the train_graph
        if hasattr(graph_data, 'edge_timestamps') and graph_data.edge_timestamps is not None:
            train_graph.edge_timestamps = edge_times[before_mask]
        elif hasattr(graph_data, 'edge_time') and graph_data.edge_time is not None:
            train_graph.edge_time = edge_times[before_mask]
        
        # Ensure node timestamps are copied
        if hasattr(graph_data, 'node_timestamps') and graph_data.node_timestamps is not None:
            train_graph.node_timestamps = graph_data.node_timestamps
        elif hasattr(graph_data, 'paper_times') and graph_data.paper_times is not None:
            train_graph.paper_times = graph_data.paper_times
        
        # Generate negative samples for testing
        # These should also respect the temporal constraint (they don't exist after threshold)
        negative_edges = self._generate_temporal_negative_edges(
            graph_data,
            time_threshold=time_threshold,
            future_window=future_window,
            num_samples=test_edges.shape[1]
        )
        
        # Prepare positive and negative examples for evaluation
        src_pos, dst_pos = test_edges[0], test_edges[1]
        src_neg, dst_neg = negative_edges[0], negative_edges[1]
        
        # Create time-based edge attributes if available
        pos_edge_attr = None
        neg_edge_attr = None
        
        # If the predictor is a TemporalPredictor, we need to include time information
        if 'TemporalPredictor' in predictor.__class__.__name__:
            # Initialize edge attributes for positive edges
            pos_edge_times = edge_times[after_mask]
            pos_edge_attr = pos_edge_times.unsqueeze(1) if pos_edge_times.dim() == 1 else pos_edge_times
            
            # For negative edges, we need to generate plausible timestamps
            # A simple approach is to use random timestamps within the future window
            if future_window is not None:
                # Random times within the future window
                neg_times = time_threshold + torch.rand(negative_edges.shape[1]) * future_window
            else:
                # Random times after the threshold (using the same range as positive edges)
                max_time = edge_times.max().item()
                neg_times = time_threshold + torch.rand(negative_edges.shape[1]) * (max_time - time_threshold)
            
            neg_edge_attr = neg_times.unsqueeze(1) if neg_times.dim() == 1 else neg_times
        
        # Get embeddings for source and destination nodes
        src_pos_emb = node_embeddings[src_pos]
        dst_pos_emb = node_embeddings[dst_pos]
        src_neg_emb = node_embeddings[src_neg]
        dst_neg_emb = node_embeddings[dst_neg]
        
        # Predict scores for positive and negative edges
        pos_scores = predictor(src_pos_emb, dst_pos_emb, pos_edge_attr)
        neg_scores = predictor(src_neg_emb, dst_neg_emb, neg_edge_attr)
        
        # Prepare ground truth labels
        pos_labels = torch.ones_like(pos_scores)
        neg_labels = torch.zeros_like(neg_scores)
        
        # Combine predictions and labels
        scores = torch.cat([pos_scores, neg_scores], dim=0)
        labels = torch.cat([pos_labels, neg_labels], dim=0)
        
        # Compute evaluation metrics
        metrics = self._compute_metrics(scores, labels)
        
        # Add number of test edges to metrics
        metrics['num_test_edges'] = test_edges.shape[1]
        metrics['num_train_edges'] = train_edges.shape[1]
        
        return metrics
    
    def _evaluate_k_fold(self,
                        predictor: BasePredictor,
                        graph_data: GraphData,
                        node_embeddings: torch.Tensor,
                        k: int = 5,
                        **kwargs) -> Dict[str, float]:
        """
        Evaluate predictor using k-fold cross-validation.
        
        Args:
            predictor: The predictor model to evaluate
            graph_data: The citation network data
            node_embeddings: Node embeddings generated by an encoder
            k: Number of folds
            **kwargs: Additional arguments
            
        Returns:
            Dictionary containing average evaluation metrics across folds
        """
        # Get all edges from the graph
        all_edges = graph_data.edge_index
        num_edges = all_edges.shape[1]
        
        # Create k folds (indices)
        indices = torch.randperm(num_edges)
        fold_size = num_edges // k
        
        # Generate negative samples once (equal to the number of positive edges)
        # This is more efficient than generating them for each fold
        self.logger.info("Generating negative samples for k-fold cross-validation")
        negative_edges = self._generate_negative_edges(
            graph_data, 
            num_samples=num_edges,
            exclude_edges=all_edges
        )
        
        # If we couldn't generate enough negative edges, adjust the number
        if negative_edges.shape[1] < num_edges:
            self.logger.warning(f"Could only generate {negative_edges.shape[1]} negative edges, adjusting fold sizes")
            num_neg_edges = negative_edges.shape[1]
            # Calculate the minimum usable edges for each class to maintain balance
            min_edges = min(num_edges, num_neg_edges)
            # Use only the first min_edges positive edges for balance
            indices = indices[:min_edges]
            num_edges = min_edges
            # Recalculate fold size
            fold_size = num_edges // k
        else:
            num_neg_edges = num_edges
        
        # Create indices for negative edges
        neg_indices = torch.randperm(num_neg_edges)
        
        # Store metrics for each fold
        fold_metrics = []
        
        # For each fold
        for fold in range(k):
            self.logger.info(f"Evaluating fold {fold+1}/{k}")
            
            try:
                # Determine test indices for this fold
                start_idx = fold * fold_size
                end_idx = (fold + 1) * fold_size if fold < k - 1 else num_edges
                test_indices = indices[start_idx:end_idx]
                
                # All other indices are training
                train_mask = torch.ones(num_edges, dtype=torch.bool)
                train_mask[test_indices] = False
                train_indices = indices[train_mask]
                
                # Create train and test edge sets
                train_edges = all_edges[:, train_indices]
                test_edges = all_edges[:, test_indices]
                
                # Create a modified graph with only training edges
                train_graph = self._create_subgraph(graph_data, train_edges)
                
                # Get negative edges for this fold
                neg_start_idx = fold * fold_size
                neg_end_idx = (fold + 1) * fold_size if fold < k - 1 else num_neg_edges
                neg_test_indices = neg_indices[neg_start_idx:neg_end_idx]
                fold_negative_edges = negative_edges[:, neg_test_indices]
                
                # Prepare positive and negative examples for evaluation
                src_pos, dst_pos = test_edges[0], test_edges[1]
                src_neg, dst_neg = fold_negative_edges[0], fold_negative_edges[1]
                
                # Create edge attributes if available (e.g., time differences)
                pos_edge_attr = None
                neg_edge_attr = None
                
                if hasattr(graph_data, 'edge_attr') and graph_data.edge_attr is not None:
                    pos_edge_attr = graph_data.edge_attr[test_indices]
                    
                    # For negative edges, we need to create placeholder attributes
                    neg_edge_attr = torch.zeros(
                        (fold_negative_edges.shape[1], graph_data.edge_attr.shape[1]),
                        device=graph_data.edge_attr.device
                    )
                
                # Get embeddings for source and destination nodes
                src_pos_emb = node_embeddings[src_pos]
                dst_pos_emb = node_embeddings[dst_pos]
                src_neg_emb = node_embeddings[src_neg]
                dst_neg_emb = node_embeddings[dst_neg]
                
                # Predict scores for positive and negative edges
                pos_scores = predictor(src_pos_emb, dst_pos_emb, pos_edge_attr)
                neg_scores = predictor(src_neg_emb, dst_neg_emb, neg_edge_attr)
                
                # Prepare ground truth labels
                pos_labels = torch.ones_like(pos_scores)
                neg_labels = torch.zeros_like(neg_scores)
                
                # Combine predictions and labels
                scores = torch.cat([pos_scores, neg_scores], dim=0)
                labels = torch.cat([pos_labels, neg_labels], dim=0)
                
                # Compute evaluation metrics
                metrics = self._compute_metrics(scores, labels)
                
                # Store metrics for this fold
                fold_metrics.append(metrics)
                
            except Exception as e:
                # Log the error but continue with other folds
                self.logger.error(f"Error in fold {fold+1}/{k}: {str(e)}")
                # Add empty metrics for this fold to maintain fold count
                fold_metrics.append({})
                continue
        
        # Compute average metrics across folds
        avg_metrics = {}
        
        # Get all metric keys
        all_keys = set()
        for metrics in fold_metrics:
            all_keys.update(metrics.keys())
        
        # Filter out non-scalar metrics and compute averages
        for key in all_keys:
            # Skip internal curve data
            if key.startswith('_'):
                continue
                
            # Try to compute average
            try:
                values = [metrics.get(key, float('nan')) for metrics in fold_metrics]
                # Filter out nan values
                values = [v for v in values if not np.isnan(v)]
                if values:
                    avg_metrics[key] = float(np.mean(values))
                else:
                    avg_metrics[key] = float('nan')
            except (TypeError, ValueError):
                # Skip non-numeric values
                pass
        
        # Store fold metrics for reference
        avg_metrics['_fold_metrics'] = fold_metrics
        avg_metrics['_num_folds'] = k
        avg_metrics['_successful_folds'] = sum(1 for m in fold_metrics if m)
        
        # Log warning if some folds failed
        if avg_metrics['_successful_folds'] < k:
            self.logger.warning(f"Only {avg_metrics['_successful_folds']}/{k} folds completed successfully")
        
        return avg_metrics
    
    def _compute_metrics(self, 
                        scores: torch.Tensor, 
                        labels: torch.Tensor) -> Dict[str, float]:
        """
        Compute evaluation metrics from prediction scores and ground truth labels.
        
        Args:
            scores: Prediction scores
            labels: Ground truth binary labels
            
        Returns:
            Dictionary of metrics
        """
        # Convert to numpy for sklearn metrics
        if isinstance(scores, torch.Tensor):
            scores = scores.detach().cpu().numpy()
        if isinstance(labels, torch.Tensor):
            labels = labels.detach().cpu().numpy()
            
        # Create a cache key based on the hash of scores and labels
        # This allows us to reuse computed metrics if the same data is passed multiple times
        cache_key = hash((scores.tobytes(), labels.tobytes()))
        
        # Check if we've already computed metrics for this data
        if cache_key in self.metrics_cache:
            self.logger.debug("Using cached metrics")
            # Move this key to the end of the list (most recently used)
            if cache_key in self.cache_keys:
                self.cache_keys.remove(cache_key)
            self.cache_keys.append(cache_key)
            # Return a deep copy to prevent modification of cached values
            import copy
            return copy.deepcopy(self.metrics_cache[cache_key])
            
        metrics = {}
        
        # AUC (Area Under ROC Curve)
        if 'auc' in self.metrics:
            try:
                metrics['auc'] = roc_auc_score(labels, scores)
                # Also compute and store ROC curve for visualization
                fpr, tpr, thresholds = roc_curve(labels, scores)
                metrics['_roc_curve'] = {
                    'fpr': fpr, 
                    'tpr': tpr, 
                    'thresholds': thresholds
                }
            except Exception as e:
                self.logger.warning(f"Error computing AUC: {str(e)}")
                metrics['auc'] = float('nan')
        
        # AP (Average Precision)
        if 'ap' in self.metrics:
            try:
                metrics['ap'] = average_precision_score(labels, scores)
                # Also compute and store PR curve for visualization
                precision, recall, thresholds = precision_recall_curve(labels, scores)
                metrics['_pr_curve'] = {
                    'precision': precision, 
                    'recall': recall, 
                    'thresholds': thresholds
                }
            except Exception as e:
                self.logger.warning(f"Error computing AP: {str(e)}")
                metrics['ap'] = float('nan')
        
        # Precision, Recall, F1 (using threshold of 0.5)
        if any(m in self.metrics for m in ['precision', 'recall', 'f1']):
            try:
                # Convert scores to binary predictions using threshold of 0.5
                predictions = (scores >= 0.5).astype(int)
                
                if 'precision' in self.metrics:
                    metrics['precision'] = precision_score(labels, predictions)
                    
                if 'recall' in self.metrics:
                    metrics['recall'] = recall_score(labels, predictions)
                    
                if 'f1' in self.metrics:
                    metrics['f1'] = f1_score(labels, predictions)
            except Exception as e:
                self.logger.warning(f"Error computing classification metrics: {str(e)}")
                for m in ['precision', 'recall', 'f1']:
                    if m in self.metrics:
                        metrics[m] = float('nan')
        
        # Precision@k and Recall@k
        for k in self.k_values:
            if f'precision@{k}' in self.metrics:
                try:
                    metrics[f'precision@{k}'] = self._precision_at_k(scores, labels, k)
                except Exception as e:
                    self.logger.warning(f"Error computing Precision@{k}: {str(e)}")
                    metrics[f'precision@{k}'] = float('nan')
                    
            if f'recall@{k}' in self.metrics:
                try:
                    metrics[f'recall@{k}'] = self._recall_at_k(scores, labels, k)
                except Exception as e:
                    self.logger.warning(f"Error computing Recall@{k}: {str(e)}")
                    metrics[f'recall@{k}'] = float('nan')
        
        # Mean Reciprocal Rank
        if 'mrr' in self.metrics:
            try:
                metrics['mrr'] = self._mean_reciprocal_rank(scores, labels)
            except Exception as e:
                self.logger.warning(f"Error computing MRR: {str(e)}")
                metrics['mrr'] = float('nan')
        
        # Cache the computed metrics (store a deep copy to prevent modification)
        import copy
        self.metrics_cache[cache_key] = copy.deepcopy(metrics)
        self.cache_keys.append(cache_key)
        
        # Enforce cache size limit (LRU eviction policy)
        if len(self.cache_keys) > self.max_cache_size:
            # Remove oldest entry
            oldest_key = self.cache_keys.pop(0)
            if oldest_key in self.metrics_cache:
                del self.metrics_cache[oldest_key]
        
        return metrics
    
    def _precision_at_k(self, 
                       scores: np.ndarray, 
                       labels: np.ndarray, 
                       k: int) -> float:
        """
        Compute Precision@k.
        
        Args:
            scores: Prediction scores
            labels: Ground truth binary labels
            k: Number of top predictions to consider
            
        Returns:
            Precision@k value
        """
        # Handle edge case: empty input
        if len(scores) == 0 or len(labels) == 0:
            self.logger.warning("Empty input for Precision@k calculation")
            return float('nan')
            
        # Handle edge case: k is larger than the number of predictions
        if k > len(scores):
            self.logger.warning(f"k ({k}) is larger than the number of predictions ({len(scores)})")
            k = len(scores)
            
        # Handle edge case: k is 0
        if k == 0:
            self.logger.warning("k is 0, returning NaN for Precision@k")
            return float('nan')
            
        # Get indices of top-k predictions
        top_k_indices = np.argsort(scores)[-k:]
        
        # Get ground truth values for top-k predictions
        top_k_ground_truth = labels[top_k_indices]
        
        # Compute precision@k
        precision_at_k = top_k_ground_truth.mean()
        
        return precision_at_k
    
    def _recall_at_k(self, 
                    scores: np.ndarray, 
                    labels: np.ndarray, 
                    k: int) -> float:
        """
        Compute Recall@k.
        
        Args:
            scores: Prediction scores
            labels: Ground truth binary labels
            k: Number of top predictions to consider
            
        Returns:
            Recall@k value
        """
        # Handle edge case: empty input
        if len(scores) == 0 or len(labels) == 0:
            self.logger.warning("Empty input for Recall@k calculation")
            return float('nan')
            
        # Handle edge case: k is larger than the number of predictions
        if k > len(scores):
            self.logger.warning(f"k ({k}) is larger than the number of predictions ({len(scores)})")
            k = len(scores)
            
        # Handle edge case: k is 0
        if k == 0:
            self.logger.warning("k is 0, returning NaN for Recall@k")
            return float('nan')
            
        # Handle edge case: no positive examples
        if np.sum(labels) == 0:
            self.logger.warning("No positive examples in ground truth for Recall@k calculation")
            return float('nan')
            
        # Get indices of top-k predictions
        top_k_indices = np.argsort(scores)[-k:]
        
        # Count true positives in top-k
        true_positives = np.sum(labels[top_k_indices])
        
        # Count all positives
        total_positives = np.sum(labels)
        
        # Compute recall@k
        recall_at_k = true_positives / total_positives
            
        return recall_at_k
    
    def _mean_reciprocal_rank(self, 
                             scores: np.ndarray, 
                             labels: np.ndarray) -> float:
        """
        Compute Mean Reciprocal Rank (MRR).
        
        Args:
            scores: Prediction scores
            labels: Ground truth binary labels
            
        Returns:
            MRR value
        """
        # Handle edge case: empty input
        if len(scores) == 0 or len(labels) == 0:
            self.logger.warning("Empty input for MRR calculation")
            return float('nan')
            
        # Handle edge case: no positive examples
        if np.sum(labels) == 0:
            self.logger.warning("No positive examples in ground truth for MRR calculation")
            return float('nan')
            
        # Sort indices by score (descending)
        sorted_indices = np.argsort(scores)[::-1]
        
        # Find ranks of positive examples
        ranks = []
        for i, idx in enumerate(sorted_indices):
            if labels[idx] == 1:
                # Rank is 1-indexed
                ranks.append(i + 1)
        
        # Compute MRR
        if ranks:
            mrr = np.mean([1.0 / r for r in ranks])
        else:
            mrr = 0.0
            
        return mrr
    
    def _create_subgraph(self, 
                        graph_data: GraphData, 
                        edges: torch.Tensor) -> GraphData:
        """
        Create a subgraph containing only the specified edges.
        
        Args:
            graph_data: Original graph data
            edges: Edge indices to include in the subgraph
            
        Returns:
            Subgraph with only the specified edges
        """
        # Create a new GraphData object with the same nodes but only the specified edges
        subgraph = GraphData(
            x=graph_data.x,
            edge_index=edges
        )
        
        # Copy node timestamps if available (using references instead of copying)
        if hasattr(graph_data, 'node_timestamps') and graph_data.node_timestamps is not None:
            subgraph.node_timestamps = graph_data.node_timestamps  # No need to copy, just reference
        elif hasattr(graph_data, 'paper_times') and graph_data.paper_times is not None:
            subgraph.paper_times = graph_data.paper_times  # No need to copy, just reference
            
        # Copy edge timestamps if available
        if hasattr(graph_data, 'edge_timestamps') and graph_data.edge_timestamps is not None:
            # More efficient approach: create a dictionary for fast lookup
            edge_dict = {}
            for i in range(graph_data.edge_index.shape[1]):
                src, dst = graph_data.edge_index[0, i].item(), graph_data.edge_index[1, i].item()
                edge_dict[(src, dst)] = i
            
            # Find indices of edges in the original graph
            edge_indices = []
            for i in range(edges.shape[1]):
                src, dst = edges[0, i].item(), edges[1, i].item()
                if (src, dst) in edge_dict:
                    edge_indices.append(edge_dict[(src, dst)])
            
            # If we found any matching edges, extract their timestamps
            if edge_indices:
                edge_indices = torch.tensor(edge_indices, device=graph_data.edge_timestamps.device)
                subgraph.edge_timestamps = graph_data.edge_timestamps[edge_indices]
            else:
                # Create empty tensor with same device and dtype
                subgraph.edge_timestamps = torch.zeros(0, device=graph_data.edge_timestamps.device, 
                                                     dtype=graph_data.edge_timestamps.dtype)
        
        elif hasattr(graph_data, 'edge_time') and graph_data.edge_time is not None:
            # Same approach for edge_time field
            edge_dict = {}
            for i in range(graph_data.edge_index.shape[1]):
                src, dst = graph_data.edge_index[0, i].item(), graph_data.edge_index[1, i].item()
                edge_dict[(src, dst)] = i
            
            edge_indices = []
            for i in range(edges.shape[1]):
                src, dst = edges[0, i].item(), edges[1, i].item()
                if (src, dst) in edge_dict:
                    edge_indices.append(edge_dict[(src, dst)])
            
            if edge_indices:
                edge_indices = torch.tensor(edge_indices, device=graph_data.edge_time.device)
                subgraph.edge_time = graph_data.edge_time[edge_indices]
            else:
                subgraph.edge_time = torch.zeros(0, device=graph_data.edge_time.device, 
                                               dtype=graph_data.edge_time.dtype)
        
        # Copy any other attributes that might be useful (metadata)
        for attr in ['topics', 'keywords', 'abstracts', 'authors']:
            if hasattr(graph_data, attr):
                setattr(subgraph, attr, getattr(graph_data, attr))  # Reference, don't copy
        
        return subgraph
    
    def _generate_negative_edges(self,
                                 graph_data: GraphData,
                                 num_samples: int,
                                 exclude_edges: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Generate negative edges that don't exist in the graph.
        
        Args:
            graph_data: The graph data
            num_samples: Number of negative edges to generate
            exclude_edges: Edges to exclude from consideration
            
        Returns:
            Tensor of negative edge indices [2, num_samples]
        """
        # Validate num_samples
        if num_samples <= 0:
            raise ValueError(f"num_samples must be positive, got {num_samples}")
            
        # Get number of nodes
        if hasattr(graph_data, 'x') and graph_data.x is not None:
            num_nodes = graph_data.x.shape[0]
        elif hasattr(graph_data, 'node_timestamps') and graph_data.node_timestamps is not None:
            num_nodes = len(graph_data.node_timestamps)
        elif hasattr(graph_data, 'paper_times') and graph_data.paper_times is not None:
            num_nodes = len(graph_data.paper_times)
        else:
            # Fallback: get max node index from edge_index + 1
            num_nodes = graph_data.edge_index.max().item() + 1
        
        # Early exit for empty or singleton graphs
        if num_nodes <= 1:
            self.logger.warning(f"Graph has {num_nodes} nodes, which is too few for edge prediction")
            return torch.zeros((2, 0), dtype=torch.long)
        
        # Use edges to exclude
        if exclude_edges is None:
            exclude_edges = graph_data.edge_index
            
        # Create a set of existing edges for efficient lookup
        # Instead of a dense matrix, use a set of tuples for memory efficiency
        existing_edges = set()
        for i in range(exclude_edges.shape[1]):
            src, dst = exclude_edges[0, i].item(), exclude_edges[1, i].item()
            if src < num_nodes and dst < num_nodes:  # Ensure indices are valid
                existing_edges.add((src, dst))
        
        # Add self-loops to existing edges
        for i in range(num_nodes):
            existing_edges.add((i, i))
            
        # Calculate maximum possible negative edges
        max_possible_edges = num_nodes * (num_nodes - 1) - len(existing_edges)
        if max_possible_edges <= 0:
            self.logger.warning("No possible negative edges: graph is fully connected")
            return torch.zeros((2, 0), dtype=torch.long)
        
        # Adjust num_samples if we're asking for more than possible
        if num_samples > max_possible_edges:
            self.logger.warning(f"Requested {num_samples} negative edges, but only {max_possible_edges} are possible")
            num_samples = max_possible_edges
            
        # Sample negative edges
        negative_edges = []
        attempts = 0
        max_attempts = min(num_samples * 10, 1000000)  # Reasonable upper limit
        
        # Use different strategies based on graph density
        graph_density = len(existing_edges) / (num_nodes * num_nodes)
        
        if graph_density > 0.5:
            # For dense graphs, use a more targeted approach
            self.logger.info("Using targeted sampling for dense graph")
            
            # Create all possible valid pairs
            valid_pairs = []
            for src in range(num_nodes):
                for dst in range(num_nodes):
                    # Skip if edge exists
                    if (src, dst) in existing_edges:
                        continue
                    
                    valid_pairs.append((src, dst))
                    
                    # Break early if we have enough pairs
                    if len(valid_pairs) >= num_samples * 2:
                        break
                
                # Break early if we have enough pairs
                if len(valid_pairs) >= num_samples * 2:
                    break
            
            # Randomly sample from valid pairs
            if valid_pairs:
                # Shuffle and take the first num_samples pairs
                random.shuffle(valid_pairs)
                negative_edges = valid_pairs[:min(num_samples, len(valid_pairs))]
        else:
            # For sparse graphs, use random sampling with rejection
            self.logger.info("Using random sampling for sparse graph")
            
            # Use a set to track sampled edges for uniqueness
            sampled_edges = set()
            
            while len(negative_edges) < num_samples and attempts < max_attempts:
                attempts += 1
                
                # Sample random nodes
                src = random.randint(0, num_nodes - 1)
                dst = random.randint(0, num_nodes - 1)
                
                # Skip if already sampled or if edge exists
                if (src, dst) in sampled_edges or (src, dst) in existing_edges:
                    continue
                
                # Add to negative edges
                negative_edges.append((src, dst))
                sampled_edges.add((src, dst))
                
                # Adaptive sampling: if we're struggling to find edges, reduce target
                if attempts > max_attempts // 2 and len(negative_edges) < num_samples // 2:
                    self.logger.warning(f"Reducing target from {num_samples} to {len(negative_edges) * 2}")
                    num_samples = len(negative_edges) * 2
        
        # Convert to tensor
        if negative_edges:
            negative_edges_tensor = torch.tensor(negative_edges, dtype=torch.long).t()
        else:
            # Return empty tensor if no valid negative edges found
            self.logger.warning("Could not find any valid negative edges")
            negative_edges_tensor = torch.zeros((2, 0), dtype=torch.long)
            
        return negative_edges_tensor
    
    def _generate_temporal_negative_edges(self,
                                        graph_data: GraphData,
                                        time_threshold: float,
                                        future_window: Optional[float] = None,
                                        num_samples: int = 1000) -> torch.Tensor:
        """
        Generate negative edges that respect temporal constraints.
        
        These are edges that don't exist in the graph after the time threshold.
        
        Args:
            graph_data: The graph data
            time_threshold: Timestamp to use as cutoff
            future_window: Optional time window after threshold to consider
            num_samples: Number of negative edges to generate
            
        Returns:
            Tensor of negative edge indices [2, num_samples]
        """
        # Validate time_threshold is a valid float
        if not isinstance(time_threshold, (int, float)):
            raise ValueError(f"time_threshold must be a number, got {type(time_threshold)}")
        
        # Validate future_window if provided
        if future_window is not None and not isinstance(future_window, (int, float)):
            raise ValueError(f"future_window must be a number if provided, got {type(future_window)}")
        
        # Validate num_samples
        if num_samples <= 0:
            raise ValueError(f"num_samples must be positive, got {num_samples}")
        
        # Get number of nodes
        if hasattr(graph_data, 'x') and graph_data.x is not None:
            num_nodes = graph_data.x.shape[0]
        elif hasattr(graph_data, 'node_timestamps') and graph_data.node_timestamps is not None:
            num_nodes = len(graph_data.node_timestamps)
        elif hasattr(graph_data, 'paper_times') and graph_data.paper_times is not None:
            num_nodes = len(graph_data.paper_times)
        else:
            # Fallback: get max node index from edge_index + 1
            num_nodes = graph_data.edge_index.max().item() + 1
        
        # Early exit for empty or singleton graphs
        if num_nodes <= 1:
            self.logger.warning(f"Graph has {num_nodes} nodes, which is too few for edge prediction")
            return torch.zeros((2, 0), dtype=torch.long)
        
        # Get edge timestamps
        if hasattr(graph_data, 'edge_timestamps') and graph_data.edge_timestamps is not None:
            edge_times = graph_data.edge_timestamps
        elif hasattr(graph_data, 'edge_time') and graph_data.edge_time is not None:
            edge_times = graph_data.edge_time
        else:
            raise ValueError("Graph data must contain edge timestamps for temporal evaluation")
        
        # Get edges
        all_edges = graph_data.edge_index
        
        # Find future edges
        if future_window is not None:
            future_mask = (edge_times > time_threshold) & (edge_times <= time_threshold + future_window)
        else:
            future_mask = edge_times > time_threshold
            
        future_edges = all_edges[:, future_mask]
        
        # Create a set of existing edges for efficient lookup
        # Instead of a dense matrix, use a set of tuples for memory efficiency
        existing_edges = set()
        for i in range(all_edges.shape[1]):
            src, dst = all_edges[0, i].item(), all_edges[1, i].item()
            existing_edges.add((src, dst))
        
        # Create a set of future edges to avoid
        future_edge_set = set()
        for i in range(future_edges.shape[1]):
            src, dst = future_edges[0, i].item(), future_edges[1, i].item()
            future_edge_set.add((src, dst))
        
        # Get node timestamps
        node_times = None
        if hasattr(graph_data, 'node_timestamps') and graph_data.node_timestamps is not None:
            node_times = graph_data.node_timestamps
        elif hasattr(graph_data, 'paper_times') and graph_data.paper_times is not None:
            node_times = graph_data.paper_times
        
        # Create a list of valid nodes (published before or at time_threshold)
        valid_nodes = None
        if node_times is not None:
            valid_nodes = torch.where(node_times <= time_threshold)[0].tolist()
        else:
            # If no node timestamps, assume all nodes are valid
            valid_nodes = list(range(num_nodes))
        
        # Early termination if not enough valid nodes
        if len(valid_nodes) < 2:
            self.logger.warning(f"Not enough valid nodes at time {time_threshold}: {len(valid_nodes)}")
            return torch.zeros((2, 0), dtype=torch.long)
        
        # Calculate maximum possible negative edges
        max_possible_edges = len(valid_nodes) * (len(valid_nodes) - 1) - len(existing_edges)
        if max_possible_edges <= 0:
            self.logger.warning("No possible negative edges: graph is fully connected")
            return torch.zeros((2, 0), dtype=torch.long)
        
        # Adjust num_samples if we're asking for more than possible
        if num_samples > max_possible_edges:
            self.logger.warning(f"Requested {num_samples} negative edges, but only {max_possible_edges} are possible")
            num_samples = max_possible_edges
        
        # Sample negative edges with time constraints
        negative_edges = []
        attempts = 0
        max_attempts = min(num_samples * 10, 1000000)  # Reasonable upper limit
        
        # Use different strategies based on graph density
        graph_density = len(existing_edges) / (num_nodes * (num_nodes - 1))
        
        if graph_density > 0.5:
            # For dense graphs, use a more targeted approach
            # Generate all possible valid pairs and filter
            self.logger.info("Using targeted sampling for dense graph")
            
            # Create a set of valid pairs
            valid_pairs = []
            for src in valid_nodes:
                for dst in valid_nodes:
                    # Skip self-loops
                    if src == dst:
                        continue
                    
                    # Skip existing edges
                    if (src, dst) in existing_edges:
                        continue
                    
                    # Apply temporal constraint if node timestamps available
                    if node_times is not None:
                        src_time = node_times[src].item()
                        dst_time = node_times[dst].item()
                        
                        # Ensure causal ordering: papers can only cite older papers
                        # For citations, the source (citing) paper must be newer than or 
                        # published at the same time as the destination (cited) paper
                        if src_time < dst_time:  # This is the temporal causality constraint
                            continue
                    
                    valid_pairs.append((src, dst))
                    
                    # Break early if we have enough pairs
                    if len(valid_pairs) >= num_samples * 2:
                        break
                
                # Break early if we have enough pairs
                if len(valid_pairs) >= num_samples * 2:
                    break
            
            # Randomly sample from valid pairs
            if valid_pairs:
                # Shuffle and take the first num_samples pairs
                random.shuffle(valid_pairs)
                negative_edges = valid_pairs[:min(num_samples, len(valid_pairs))]
        else:
            # For sparse graphs, use random sampling with rejection
            self.logger.info("Using random sampling for sparse graph")
            
            # Use a set to track sampled edges for uniqueness
            sampled_edges = set()
            
            while len(negative_edges) < num_samples and attempts < max_attempts:
                attempts += 1
                
                # Sample random nodes from valid nodes
                src = random.choice(valid_nodes)
                dst = random.choice(valid_nodes)
                
                # Skip self-loops
                if src == dst:
                    continue
                
                # Skip if already sampled
                if (src, dst) in sampled_edges:
                    continue
                
                # Skip existing edges
                if (src, dst) in existing_edges:
                    continue
                
                # Skip future edges (we don't want to predict edges that actually form)
                if (src, dst) in future_edge_set:
                    continue
                
                # Apply temporal constraint if node timestamps available
                if node_times is not None:
                    src_time = node_times[src].item()
                    dst_time = node_times[dst].item()
                    
                    # Ensure causal ordering: papers can only cite older papers
                    # For citations, the source (citing) paper must be newer than or 
                    # published at the same time as the destination (cited) paper
                    if src_time < dst_time:  # This is the temporal causality constraint
                        continue
                
                # Add to negative edges
                negative_edges.append((src, dst))
                sampled_edges.add((src, dst))
                
                # Adaptive sampling: if we're struggling to find edges, reduce target
                if attempts > max_attempts // 2 and len(negative_edges) < num_samples // 2:
                    self.logger.warning(f"Reducing target from {num_samples} to {len(negative_edges) * 2}")
                    num_samples = len(negative_edges) * 2
        
        # Convert to tensor
        if negative_edges:
            negative_edges_tensor = torch.tensor(negative_edges, dtype=torch.long).t()
        else:
            # Return empty tensor if no valid negative edges found
            self.logger.warning("Could not find any valid negative edges that respect temporal constraints")
            negative_edges_tensor = torch.zeros((2, 0), dtype=torch.long)
        
        return negative_edges_tensor
    
    def compare_predictors(self,
                          predictors: List[BasePredictor],
                          predictor_names: List[str],
                          graph_data: GraphData,
                          node_embeddings: torch.Tensor,
                          split_type: str = 'random',
                          test_ratio: float = 0.2,
                          time_threshold: Optional[float] = None,
                          future_window: Optional[float] = None,
                          k_fold: int = 5,
                          save_results: bool = True,
                          **kwargs) -> Dict[str, Dict[str, float]]:
        """
        Compare multiple predictors and save the results.
        
        Args:
            predictors: List of predictor models to compare
            predictor_names: List of names for the predictors
            graph_data: The citation network data
            node_embeddings: Node embeddings generated by an encoder
            split_type: Type of data split ('random', 'temporal', 'k-fold')
            test_ratio: Ratio of edges to use for testing (for 'random' split)
            time_threshold: Timestamp to use as cutoff (for 'temporal' split)
            future_window: Time window after threshold to predict (for 'temporal' split)
            k_fold: Number of folds (for 'k-fold' split)
            save_results: Whether to save the results to file
            **kwargs: Additional arguments for specific evaluation methods
            
        Returns:
            Dictionary containing comparison results
        """
        comparison_results = {}
        
        for predictor, name in zip(predictors, predictor_names):
            self.logger.info(f"Evaluating {name}")
            metrics = self.evaluate_predictor(predictor, graph_data, node_embeddings, split_type, test_ratio, time_threshold, future_window, k_fold, **kwargs)
            comparison_results[name] = metrics
        
        if save_results:
            self._save_comparison_results(comparison_results)
        
        return comparison_results
    
    def _save_comparison_results(self,
                                 comparison_results: Dict[str, Dict[str, float]]) -> None:
        """
        Save comparison results to file.
        
        Args:
            comparison_results: Dictionary containing comparison results
        """
        import json
        
        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Create serializable format
        serializable_results = {}
        for name, metrics in comparison_results.items():
            serializable_metrics = {}
            for k, v in metrics.items():
                if isinstance(v, (list, dict)):
                    serializable_metrics[k] = json.dumps(v)
                else:
                    serializable_metrics[k] = v
            serializable_results[name] = serializable_metrics
        
        # Save to JSON
        results_file = os.path.join(self.output_dir, 'comparison_results.json')
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
            
        self.logger.info(f"Comparison results saved to {results_file}")
    
    def _visualize_ablation_study(self,
                                ablation_results: Dict[str, Dict[str, Dict[str, float]]],
                                predictor_name: str,
                                target_metric: str,
                                split_type: str) -> None:
        """
        Create visualizations for the ablation study.
        
        Args:
            ablation_results: Dictionary containing ablation results
            predictor_name: Name of the predictor class
            target_metric: Metric used for optimization
            split_type: Type of data split used
        """
        import matplotlib.pyplot as plt
        
        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)
        
        # For each parameter
        for param_name, param_results in ablation_results.items():
            # Create a line plot
            plt.figure(figsize=(10, 6))
            
            # Extract values and metrics
            values = []
            metrics = []
            
            for value, result in param_results.items():
                # Try to convert string value to float if possible
                try:
                    val = float(value)
                except ValueError:
                    val = value
                    
                values.append(val)
                metrics.append(result.get(target_metric, 0))
            
            # Sort by values if they are numeric
            if all(isinstance(v, (int, float)) for v in values):
                sorted_indices = np.argsort(values)
                values = [values[i] for i in sorted_indices]
                metrics = [metrics[i] for i in sorted_indices]
                
                # Plot line
                plt.plot(values, metrics, 'o-', linewidth=2, markersize=8)
                
                # Add value labels
                for x, y in zip(values, metrics):
                    plt.text(x, y + 0.01, f'{y:.3f}', ha='center', va='bottom')
            else:
                # For non-numeric values, use a bar chart
                plt.bar(values, metrics)
                
                # Add value labels
                for i, (x, y) in enumerate(zip(values, metrics)):
                    plt.text(i, y + 0.01, f'{y:.3f}', ha='center', va='bottom')
            
            # Set up axes and labels
            plt.xlabel(param_name)
            plt.ylabel(target_metric)
            plt.title(f'Ablation Study: {predictor_name} - {param_name} ({split_type.capitalize()} Split)')
            plt.grid(True, linestyle='--', alpha=0.7)
            
            # Adjust layout and save
            plt.tight_layout()
            plt.savefig(os.path.join(self.output_dir, f'ablation_{predictor_name}_{param_name}_{split_type}.png'))
            plt.close()
            
            self.logger.info(f"Ablation plot saved to {os.path.join(self.output_dir, f'ablation_{predictor_name}_{param_name}_{split_type}.png')}")
        
        # Create a summary plot with all parameters
        self._create_ablation_summary_plot(ablation_results, predictor_name, target_metric, split_type)

    def _create_ablation_summary_plot(self,
                                    ablation_results: Dict[str, Dict[str, Dict[str, float]]],
                                    predictor_name: str,
                                    target_metric: str,
                                    split_type: str) -> None:
        """
        Create a summary plot for the ablation study.
        
        Args:
            ablation_results: Dictionary containing ablation results
            predictor_name: Name of the predictor class
            target_metric: Metric used for optimization
            split_type: Type of data split used
        """
        import matplotlib.pyplot as plt
        
        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Extract all metrics from the ablation results
        all_metrics = set()
        for param_name, param_results in ablation_results.items():
            all_metrics.update(param_results.keys())
        
        # Create a line plot for each metric
        for metric in all_metrics:
            plt.figure(figsize=(10, 6))
            
            # Extract values and metrics
            values = []
            metrics = []
            
            for param_name, param_results in ablation_results.items():
                value = param_name
                metric_value = param_results.get(metric, 0)
                
                values.append(value)
                metrics.append(metric_value)
            
            # Sort by values if they are numeric
            if all(isinstance(v, (int, float)) for v in values):
                sorted_indices = np.argsort(values)
                values = [values[i] for i in sorted_indices]
                metrics = [metrics[i] for i in sorted_indices]
                
                # Plot line
                plt.plot(values, metrics, 'o-', linewidth=2, markersize=8, label=param_name)
            
            # Set up axes and labels
            plt.xlabel(metric)
            plt.ylabel(target_metric)
            plt.title(f'Ablation Study: {predictor_name} - {metric} ({split_type.capitalize()} Split)')
            plt.grid(True, linestyle='--', alpha=0.7)
            plt.legend()
            
            # Adjust layout and save
            plt.tight_layout()
            plt.savefig(os.path.join(self.output_dir, f'ablation_{predictor_name}_{metric}_{split_type}.png'))
            plt.close()
            
            self.logger.info(f"Ablation plot saved to {os.path.join(self.output_dir, f'ablation_{predictor_name}_{metric}_{split_type}.png')}")

    def visualize_results(self, 
                         metrics: Dict[str, float],
                         predictor_name: str = 'Predictor',
                         output_file: Optional[str] = None) -> None:
        """
        Visualize evaluation results, including ROC and PR curves.
        
        Args:
            metrics: Dictionary of evaluation metrics
            predictor_name: Name of the predictor for plot titles
            output_file: Path to save the visualization (if None, will show the plot)
        """
        import matplotlib.pyplot as plt
        from matplotlib.gridspec import GridSpec
        
        # Create figure with subplots
        fig = plt.figure(figsize=(15, 10))
        gs = GridSpec(2, 3, figure=fig)
        
        # ROC curve
        if '_roc_curve' in metrics:
            ax_roc = fig.add_subplot(gs[0, 0])
            roc_data = metrics['_roc_curve']
            ax_roc.plot(roc_data['fpr'], roc_data['tpr'], 'b-', lw=2)
            ax_roc.plot([0, 1], [0, 1], 'k--', lw=2)
            ax_roc.set_xlim([0.0, 1.0])
            ax_roc.set_ylim([0.0, 1.05])
            ax_roc.set_xlabel('False Positive Rate')
            ax_roc.set_ylabel('True Positive Rate')
            ax_roc.set_title(f'ROC Curve (AUC = {metrics.get("auc", "N/A"):.3f})')
            ax_roc.grid(True)
        
        # PR curve
        if '_pr_curve' in metrics:
            ax_pr = fig.add_subplot(gs[0, 1])
            pr_data = metrics['_pr_curve']
            ax_pr.plot(pr_data['recall'], pr_data['precision'], 'r-', lw=2)
            ax_pr.set_xlim([0.0, 1.0])
            ax_pr.set_ylim([0.0, 1.05])
            ax_pr.set_xlabel('Recall')
            ax_pr.set_ylabel('Precision')
            ax_pr.set_title(f'Precision-Recall Curve (AP = {metrics.get("ap", "N/A"):.3f})')
            ax_pr.grid(True)
        
        # Metrics table
        ax_metrics = fig.add_subplot(gs[0, 2])
        ax_metrics.axis('tight')
        ax_metrics.axis('off')
        
        # Filter metrics for display (exclude internal data)
        display_metrics = {k: v for k, v in metrics.items() if not k.startswith('_')}
        
        # Format metrics for table
        metric_names = []
        metric_values = []
        for k, v in sorted(display_metrics.items()):
            metric_names.append(k)
            if isinstance(v, float):
                metric_values.append(f"{v:.4f}")
            else:
                metric_values.append(str(v))
        
        # Create table
        table_data = [metric_values]
        table = ax_metrics.table(
            cellText=table_data,
            rowLabels=['Value'],
            colLabels=metric_names,
            loc='center',
            cellLoc='center'
        )
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 1.5)
        ax_metrics.set_title(f'Evaluation Metrics for {predictor_name}')
        
        # Precision@k and Recall@k plots
        ax_pk = fig.add_subplot(gs[1, 0])
        ax_rk = fig.add_subplot(gs[1, 1])
        
        # Extract Precision@k and Recall@k values
        k_values = []
        pk_values = []
        rk_values = []
        
        for k, v in metrics.items():
            if k.startswith('precision@') and not np.isnan(v):
                try:
                    k_val = int(k.split('@')[1])
                    k_values.append(k_val)
                    pk_values.append(v)
                except (ValueError, IndexError):
                    pass
        
        # Sort by k
        pk_data = sorted(zip(k_values, pk_values))
        k_values = [k for k, _ in pk_data]
        pk_values = [v for _, v in pk_data]
        
        # Plot Precision@k
        if pk_values:
            ax_pk.plot(k_values, pk_values, 'go-', lw=2)
            ax_pk.set_xlabel('k')
            ax_pk.set_ylabel('Precision@k')
            ax_pk.set_title('Precision@k')
            ax_pk.grid(True)
        
        # Extract Recall@k values
        k_values = []
        rk_values = []
        
        for k, v in metrics.items():
            if k.startswith('recall@') and not np.isnan(v):
                try:
                    k_val = int(k.split('@')[1])
                    k_values.append(k_val)
                    rk_values.append(v)
                except (ValueError, IndexError):
                    pass
        
        # Sort by k
        rk_data = sorted(zip(k_values, rk_values))
        k_values = [k for k, _ in rk_data]
        rk_values = [v for _, v in rk_data]
        
        # Plot Recall@k
        if rk_values:
            ax_rk.plot(k_values, rk_values, 'mo-', lw=2)
            ax_rk.set_xlabel('k')
            ax_rk.set_ylabel('Recall@k')
            ax_rk.set_title('Recall@k')
            ax_rk.grid(True)
        
        # Add summary text
        ax_summary = fig.add_subplot(gs[1, 2])
        ax_summary.axis('off')
        
        summary_text = f"Summary for {predictor_name}:\n\n"
        
        if 'auc' in metrics:
            summary_text += f"AUC: {metrics['auc']:.4f}\n"
        if 'ap' in metrics:
            summary_text += f"AP: {metrics['ap']:.4f}\n"
        if 'f1' in metrics:
            summary_text += f"F1: {metrics['f1']:.4f}\n"
        if 'precision' in metrics:
            summary_text += f"Precision: {metrics['precision']:.4f}\n"
        if 'recall' in metrics:
            summary_text += f"Recall: {metrics['recall']:.4f}\n"
        if 'mrr' in metrics:
            summary_text += f"MRR: {metrics['mrr']:.4f}\n"
        
        ax_summary.text(0.1, 0.5, summary_text, fontsize=12, va='center')
        
        # Adjust layout
        plt.tight_layout()
        
        # Save or show the plot
        if output_file:
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            self.logger.info(f"Visualization saved to {output_file}")
        else:
            plt.show()
        
        plt.close(fig) 
